---
title: "Full"
author: "AmyLiu"
date: "2023-03-21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Standardized Data
```{r}
# Read in the datasets
test <- read.csv("test_stand.csv")
test <- subset(test, select = -c(1))
train <- read.csv("train_stand.csv")
train <- subset(train, select = -c(1))
```

## 1. Logistic Regression Model - Standardized
**Logistic regression has training error of 0.072, a testing error of 0.052.**
**Applying logistic regression on all features results in different significance for the variables in the spam data set. 25 features from training set are significant, and 18 of them from testing set are significant. Applying logistic regression on significant features result in a confusion matrix, and we got a 0.076 error rate on the training set and 0.295 error rate on the testing set. However, the previous testing without using significant features result in a smaller and significant error rate. **
```{r}
# Logistic Regression model

#install.packages("dplyr")
#install.packages("rlist")
library(dplyr)
library(rlist)
train_model <- glm(classes ~., family=binomial, data=train)
test_model <- glm(classes ~., family=binomial, data=test)

# Confusion matrix
Probs_raw_train = predict(train_model, type='response')
Probs_raw_test = predict(test_model, type='response')

Pred_raw_train = ifelse(Probs_raw_train>0.5, 1, 0)
table(Pred_raw_train, train$classes)
mean(Pred_raw_train != train$classes)

Pred_raw_test = ifelse(Probs_raw_test>0.5, 1, 0)
table(Pred_raw_test, test$classes)
mean(Pred_raw_test != test$classes)

# find significant features
p_train = as.list(summary(train_model)$coefficients[,4])
train_view <- data.frame(p_train)
sig_fea_train = list()
col_names_train = colnames(train_view)
for (x in 1:58) {
  if (train_view[,x] < 0.05) {
    sig_fea_train <- append(sig_fea_train, col_names_train[x]);
  }
}
p_test = as.list(summary(test_model)$coefficients[,4])
test_view <- data.frame(p_test)
sig_fea_test = list()
col_names_test = colnames(test_view)
for (x in 1:58) {
  if (test_view[,x] < 0.05) {
    sig_fea_test <- append(sig_fea_test, col_names_test[x]);
  }
}
```

```{r}
# Repeat logistic regresson on significant features
train_model_sig <- glm(classes ~ word_freq_3d+word_freq_our+ word_freq_remove+word_freq_internet+word_freq_order+ word_freq_receive+word_freq_free+word_freq_business+word_freq_you+ word_freq_credit+word_freq_your+word_freq_000+word_freq_hp+ word_freq_george+word_freq_meeting+word_freq_project+word_freq_re+ word_freq_edu+word_freq_table+char_freq_.+char_freq_..3+ char_freq_..4+capital_run_length_average+ capital_run_length_longest+capital_run_length_total, family=binomial, data=train)

test_model_sig <- glm(classes ~ word_freq_our+word_freq_remove+word_freq_free+word_freq_email+word_freq_your+word_freq_money+word_freq_hp+word_freq_george+word_freq_415+word_freq_technology+word_freq_1999+word_freq_project+word_freq_re+word_freq_edu+char_freq_.+char_freq_..4+char_freq_..5+capital_run_length_longest, family=binomial, data=test)

```

```{r}
# Confusion matrix
Probs_train_sig = predict(train_model_sig, type='response')
Probs_test_sig = predict(test_model_sig, type='response')

Pred_trend_train = ifelse(Probs_train_sig>0.5, 1, 0)
table(Pred_trend_train, train$classes)
mean(Pred_trend_train != train$classes)

Pred_trend_test = ifelse(Probs_test_sig>0.5, 1, 0)
table(Pred_trend_test, test$classes)
mean(Pred_trend_test != test$classes)
```


## 2. LDA & QDA
### (1) Linear Discriminant Analysis Model
**The LDA model with 57 variables results in a testing error of 0.1. We also try applying PCA before applying LDA, but a rank of 57 results in a testing error of about 0.2. After testing out different ranks, PCA results in a testing error of 0.18.**
```{r}
# read in data
library("MASS")
true = test$classes
```

```{r}
# train LDA
lda.model = lda(classes ~., data = train)
summary(lda.model)
```

```{r}
# training error
training_res = predict(lda.model, train)$class
print("Training error: ")
mean(training_res != train$classes)
```

```{r}
# testing error
lda.predicted = predict(lda.model,test)$class
lda.error = mean(lda.predicted != true) # take mean of number of mismatching across all labels
cat("Test error rate of model is :", lda.error)
```

```{r}
# Try PCA
pca.train = prcomp(subset(train, select = -c(classes)), scale = TRUE, rank=12)$x
pca.train <- as.data.frame(pca.train)
classes = train$classes
pca.train$classes = classes # new df with 26 independent vars and 1 dependent var

lda.pca.model =lda(classes ~ ., data = pca.train, method = "mle")
summary(lda.pca.model)
```

```{r}
# training error
training_res = predict(lda.pca.model, pca.train)$class
print("Training error: ")
mean(training_res != train$classes)
```

```{r}
# prepare test data - PCA everything except classes
pca.test = prcomp(subset(test, select = -c(classes)), scale = TRUE, rank=12)$x
pca.test <- as.data.frame(pca.test)
pca.test$classes = true 

lda.pca.predicted = predict(lda.pca.model, pca.test)$class
lda.pca.error = mean(lda.pca.predicted != true) # take mean of number of mismatching across all labels
cat("Test error rate of model is :", lda.pca.error)
```

### (2) Quadratic Discriminant Analysis
**From normal QDA process, we get an error rate of 0.509, which is relevantly high. After applying PCA to the standardized data, the testing error reduced to be 0.154**

```{r}
# Train QDA
library("MASS")
true = test$classes
qda_stand = qda(classes ~., data = train)
qda_pred = predict(qda_stand, train)
pred_val_qda = qda_pred$class
test_error_qda = mean(pred_val_qda != true)
cat("Test error rate of model is: ", test_error_qda)
```

```{r}
# Apply PCA
pca_train <- prcomp(train, rank = 6)$x
pca_train <- as.data.frame(pca_train)
pca_train$classes <- train$classes
pca_test = prcomp(test, rank = 6)$x
pca_test <- as.data.frame(pca_test)
pca_test$classes <- test$classes

# QDA with PCA
qda_model_pca = qda(classes ~., data = pca_train, method = "mle")
train_pred_pca = predict(qda_model_pca, pca_train)$class
train_error = mean(train_pred_pca != train$classes)
test_pred_pca = predict(qda_model_pca, pca_test)$class
test_error = mean(test_pred_pca != test$classes)
cat("Train error rate: ", train_error, "\n")
cat("Test error rate: ", test_error)
```


## 4. Tree-based Models
**Using the standardized data, two types of tree-based models were tested. Bagging with 100 trees and m = 57 results in testing MSE of 0.043, and random forest with m=8 and 100 trees results in testing MSE of 0.036. **

```{r}
# read in data
library(randomForest)
```

### (1) Bagging
```{r}
# Bagging (Random forests with m=p) and 100 trees
bag.model = randomForest(classes~., data=train,
                           mtry=(ncol(train)-1),importance=TRUE, ntree=100)
summary(bag.model)
```

```{r}
# Prediction on training set for bagging
training_res = predict(bag.model, train)
cat("Training MSE: ", mean((training_res-train$classes)^2))
```

```{r}
# Prediction on test set for bagging
bag.predicted = predict(bag.model, test)

# Plot prediction performance
plot(bag.predicted, true)
abline (0,1)

# MSE on test set
cat("Testing MSE: ", mean((bag.predicted-true)^2))
```

### (2) Random Forest
```{r}
# Random forests with m=sqrt(p) and 100 trees. p=57, let m be 8
RF.model = randomForest(classes~., data=train,
                           mtry=8,importance=TRUE, ntree=100)
summary(RF.model)
```

```{r}
# Prediction on training set for bagging
training_res = predict(RF.model, train)
cat("Training MSE: ", mean((training_res-train$classes)^2))
```

```{r}
# Prediction on test set
RF.predicted = predict(RF.model, test)

# Plot prediction performance
plot(RF.predicted, true)
abline(0,1)

# MSE on test set
cat("Testing MSE: ", mean((RF.predicted-true)^2))
```


-------------------------

# Log transformed Data

```{r}
# Read in the datasets
test <- read.csv("test_log.csv")
test <- subset(test, select = -c(1))
train <- read.csv("train_log.csv")
train <- subset(train, select = -c(1))
```

## 1. Logistic Regression Model - Log transformed
**Logistic Regression has a training error of 0.0577, a testing error of 0.0404**
**Applying logistic regression on all features results in different significance for the variables in the spam data set. 25 features from training set are significant, and 15 of them from testing set are significant. Applying logistic regression on significant features result in a confusion matrix, and we got a 0.062 error rate on the training set and 0.048 error rate on the testing set. However, the original testing process result in a smaller error rate.**
```{r}
# Logistic Regression model
train_model <- glm(classes ~., family=binomial, data=train)
test_model <- glm(classes ~., family=binomial, data=test)

# Confusion matrix
Probs_raw_train = predict(train_model, type='response')
Probs_raw_test = predict(test_model, type='response')

Pred_raw_train = ifelse(Probs_raw_train>0.5, 1, 0)
table(Pred_raw_train, train$classes)
mean(Pred_raw_train != train$classes)

Pred_raw_test = ifelse(Probs_raw_test>0.5, 1, 0)
table(Pred_raw_test, test$classes)
mean(Pred_raw_test != test$classes)

# find significant features
p_train = as.list(summary(train_model)$coefficients[,4])
train_view <- data.frame(p_train)
sig_fea_train = list()
col_names_train = colnames(train_view)
for (x in 1:58) {
  if (train_view[,x] < 0.05) {
    sig_fea_train <- append(sig_fea_train, col_names_train[x]);
  }
}
p_test = as.list(summary(test_model)$coefficients[,4])
test_view <- data.frame(p_test)
sig_fea_test = list()
col_names_test = colnames(test_view)
for (x in 1:58) {
  if (test_view[,x] < 0.05) {
    sig_fea_test <- append(sig_fea_test, col_names_test[x]);
  }
}
```

```{r}
# Repeat logistic regresson on significant features
train_model_sig_log <- glm(classes ~ word_freq_our+word_freq_remove+word_freq_internet+word_freq_receive+word_freq_people+word_freq_free+word_freq_business+word_freq_credit+word_freq_your+word_freq_000+word_freq_money+word_freq_hp+word_freq_george+word_freq_650+word_freq_data+word_freq_85+word_freq_1999+word_freq_meeting+word_freq_original+word_freq_re+word_freq_edu+char_freq_.+char_freq_..3+char_freq_..4+capital_run_length_total, family=binomial, data=train)

test_model_sig_log <- glm(classes ~ word_freq_our+word_freq_remove+word_freq_internet+word_freq_free+word_freq_money+word_freq_hp+word_freq_george+word_freq_415+word_freq_meeting+word_freq_project+word_freq_edu+char_freq_.+char_freq_..3+char_freq_..4+capital_run_length_total, family=binomial, data=test)

```

```{r}
# Confusion matrix
Probs_train_sig = predict(train_model_sig_log, type='response')
Probs_test_sig = predict(test_model_sig_log, type='response')

Pred_trend_train = ifelse(Probs_train_sig>0.5, 1, 0)
table(Pred_trend_train, train$classes)
mean(Pred_trend_train != train$classes)

Pred_trend_test = ifelse(Probs_test_sig>0.5, 1, 0)
table(Pred_trend_test, test$classes)
mean(Pred_trend_test != test$classes)
```


## 2. LDA & QDA
### (1) Linear Discriminant Analysis Model
**The LDA model with 57 variables results in a testing error of 0.06518905. After testing out different ranks, PCA results in a testing error of 0.808, at rank 40. This is likely due to overfitting.**
```{r}
# read in data
library("MASS")
true = test$classes
```

```{r}
# train LDA
lda.model = lda(classes ~., data = train)
summary(lda.model)
```

```{r}
# training error
training_res = predict(lda.model, train)$class
print("Training error: ")
mean(training_res != train$classes)
```

```{r}
# testing error
lda.predicted = predict(lda.model,test)$class
lda.error = mean(lda.predicted != true) # take mean of number of mismatching across all labels
cat("Test error rate of model is :", lda.error)
```

```{r}
# Try PCA
pca.train = prcomp(subset(train, select = -c(classes)), scale = TRUE, rank=40)$x
pca.train <- as.data.frame(pca.train)
classes = train$classes
pca.train$classes = classes # new df with 26 independent vars and 1 dependent var

lda.pca.model =lda(classes ~ ., data = pca.train, method = "mle")
summary(lda.pca.model)
```

```{r}
# training error
training_res = predict(lda.pca.model, pca.train)$class
print("Training error: ")
mean(training_res != train$classes)
```

```{r}
# prepare test data - PCA everything except classes
pca.test = prcomp(subset(test, select = -c(classes)), scale = TRUE, rank=40)$x
pca.test <- as.data.frame(pca.test)
pca.test$classes = true # new df with 26 independent vars and 1 dependent var

lda.pca.predicted = predict(lda.pca.model, pca.test)$class
lda.pca.error = mean(lda.pca.predicted != true) # take mean of number of mismatching across all labels
cat("Test error rate of model is :", lda.pca.error)
```

### (2) Quadratic Discriminant Analysis
**From normal qda process, we get an error rate of 0.5076622, which is relevantly high. After applying PCA to the log transformed data, the test error is lowered to 0.416.**

```{r}
# Train QDA
true = test$classes
qda_stand = qda(classes ~., data = train)
qda_pred = predict(qda_stand, train)
pred_val_qda = qda_pred$class
test_error_qda = mean(pred_val_qda != true)
cat("\nTest error rate of model is: ", test_error_qda)
```

```{r}
# Apply PCA
pca_train <- prcomp(train, rank = 14)$x
pca_train <- as.data.frame(pca_train)
pca_train$classes <- train$classes
pca_test = prcomp(test, rank = 14)$x
pca_test <- as.data.frame(pca_test)
pca_test$classes <- test$classes

# QDA with PCA
qda_model_pca = qda(classes ~., data = pca_train, method = "mle")
train_pred_pca = predict(qda_model_pca, pca_train)$class
train_error = mean(train_pred_pca != train$classes)
test_pred_pca = predict(qda_model_pca, pca_test)$class
test_error = mean(test_pred_pca != test$classes)
cat("Train error rate: ", train_error, "\n")
cat("Test error rate: ", test_error)
```



## 4. Tree-based Models
**Using the log transformed data, two types of tree-based models were tested. Bagging with 100 trees and m = 57 results in testing MSE of 0.03030597, and random forest with m=8 and 100 trees results in testing MSE of 0.02653687. **

### (1) Bagging
```{r}
# Bagging (Random forests with m=p) and 100 trees
bag.model = randomForest(classes~., data=train,
                           mtry=(ncol(train)-1),importance=TRUE, ntree=100)
summary(bag.model)
```

```{r}
# Prediction on training set for bagging
training_res = predict(bag.model, train)
cat("Training MSE: ", mean((training_res-train$classes)^2))
```

```{r}
# Prediction on test set for bagging
bag.predicted = predict(bag.model, test)

# Plot prediction performance
plot(bag.predicted, true)
abline (0,1)

# MSE on test set
cat("Testing MSE: ", mean((bag.predicted-true)^2))
```

### (2) Random Forest
```{r}
# Random forests with m=sqrt(p) and 100 trees. p=57, let m be 8
RF.model = randomForest(classes~., data=train,
                           mtry=8,importance=TRUE, ntree=100)
summary(RF.model)
```

```{r}
# Prediction on training set for bagging
training_res = predict(RF.model, train)
cat("Training MSE: ", mean((training_res-train$classes)^2))
```

```{r}
# Prediction on test set
RF.predicted = predict(RF.model, test)

# Plot prediction performance
plot(RF.predicted, true)
abline(0,1)

# MSE on test set
cat("Testing MSE: ", mean((RF.predicted-true)^2))
```

-----------------------
# Discretized Data
```{r}
# Read in the datasets
test <- read.csv("test_dis.csv")
test <- subset(test, select = -c(1))
train <- read.csv("train_dis.csv")
train <- subset(train, select = -c(1))
```

## 1. Logistic Regression Model - Discretized
**Applying logistic regression on all features results in different significance for the variables in the spam data set. 28 features from training set are significant, and 13 of them from testing set are significant. Applying logistic regression on significant features result in a confusion matrix, and we got a 0.067 error rate on the training set and 0.07 error rate on the testing set. However, the original testing process result in a slightly smaller error rate. **
```{r}
# Logistic Regression model
train_model <- glm(classes ~., family=binomial, data=train)
test_model <- glm(classes ~., family=binomial, data=test)

# Confusion matrix
Probs_raw_train = predict(train_model, type='response')
Probs_raw_test = predict(test_model, type='response')

Pred_raw_train = ifelse(Probs_raw_train>0.5, 1, 0)
table(Pred_raw_train, train$classes)
mean(Pred_raw_train != train$classes)

Pred_raw_test = ifelse(Probs_raw_test>0.5, 1, 0)
table(Pred_raw_test, test$classes)
mean(Pred_raw_test != test$classes)

# find significant features
p_train = as.list(summary(train_model)$coefficients[,4])
train_view <- data.frame(p_train)
sig_fea_train = list()
col_names_train = colnames(train_view)
for (x in 1:55) {
  if (train_view[,x] < 0.05) {
    sig_fea_train <- append(sig_fea_train, col_names_train[x]);
  }
}
p_test = as.list(summary(test_model)$coefficients[,4])
test_view <- data.frame(p_test)
sig_fea_test = list()
col_names_test = colnames(test_view)
for (x in 1:55) {
  if (test_view[,x] < 0.05) {
    sig_fea_test <- append(sig_fea_test, col_names_test[x]);
  }
}
```

```{r}
# Repeat logistic regresson on significant features
train_model_sig_dis <- glm(classes ~ word_freq_our+word_freq_remove+word_freq_internet+word_freq_mail+word_freq_receive+word_freq_people+word_freq_report+word_freq_addresses+word_freq_free+word_freq_business+word_freq_email+word_freq_credit+word_freq_your+word_freq_000+word_freq_money+word_freq_hp+word_freq_george+word_freq_650+word_freq_1999+word_freq_meeting+word_freq_original+word_freq_project+word_freq_re+word_freq_edu+word_freq_conference+char_freq_..3+char_freq_..4+char_freq_..5, family=binomial, data=train)

test_model_sig_dis <- glm(classes ~ word_freq_our+word_freq_remove+word_freq_internet+word_freq_free+word_freq_your+word_freq_money+word_freq_hp+word_freq_george+word_freq_meeting+word_freq_project+word_freq_edu+char_freq_..3+char_freq_..4, family=binomial, data=test)

```

```{r}
# Confusion matrix
Probs_train_sig = predict(train_model_sig_dis, type='response')
Probs_test_sig = predict(test_model_sig_dis, type='response')

Pred_trend_train = ifelse(Probs_train_sig>0.5, 1, 0)
table(Pred_trend_train, train$classes)
mean(Pred_trend_train != train$classes)

Pred_trend_test = ifelse(Probs_test_sig>0.5, 1, 0)
table(Pred_trend_test, test$classes)
mean(Pred_trend_test != test$classes)
```


## 2. LDA & QDA
### (1) Linear Discriminant Analysis Model
**Since variables 55, 56, 57 are all 1, they provide no information in classification, so we can drop these variables. The LDA model with 54 variables results in a testing error of 0.08409387. We also try applying PCA before applying LDA. After testing out different ranks, PCA results in a testing error of 0.68 for rank 20. This is likely due to overfitting.**
```{r}
# train LDA
# Since variables 55, 56, 57 are all 1, they provide no information in classification, so we can drop these variables
train <- subset(train, select=(-c(55,56,57)))
lda.model = lda(classes ~., data = train)
summary(lda.model)
```

```{r}
# training error
training_res = predict(lda.model, train)$class
print("Training error: ")
mean(training_res != train$classes)
```

```{r}
# testing error
lda.predicted = predict(lda.model,test)$class
lda.error = mean(lda.predicted != true) # take mean of number of mismatching across all labels
cat("Test error rate of model is :", lda.error)
```

```{r}
# Try PCA
pca.train = prcomp(subset(train, select = -c(classes)), scale = TRUE, rank=20)$x
pca.train <- as.data.frame(pca.train)
classes = train$classes
pca.train$classes = classes # new df with 26 independent vars and 1 dependent var

lda.pca.model =lda(classes ~ ., data = pca.train, method = "mle")
summary(lda.pca.model)
```

```{r}
# training error
training_res = predict(lda.pca.model, pca.train)$class
print("Training error: ")
mean(training_res != train$classes)
```

```{r}
# prepare test data - PCA everything except classes
pca.test = prcomp(subset(test, select = -c(classes)), rank=20)$x
pca.test <- as.data.frame(pca.test)
pca.test$classes = true # new df with 26 independent vars and 1 dependent var

lda.pca.predicted = predict(lda.pca.model, pca.test)$class
lda.pca.error = mean(lda.pca.predicted != true) # take mean of number of mismatching across all labels
cat("Test error rate of model is :", lda.pca.error)
```

### (2) Quadratic Discriminant Analysis
**When applying normal qda to discretized data, we found that it has rank deficiency in group 0, which means that either this data has zero variance or it is perfectly collinear with another group, thus we cannot process a qda normaly, and we try PCA below. PCA gives a result of 0.093 error rate.**

```{r}
# Train QDA
try(qda(classes ~., data = train))
```

```{r}
# Apply PCA
pca_train <- prcomp(train, rank = 10)$x
pca_train <- as.data.frame(pca_train)
pca_train$classes <- train$classes
pca_test = prcomp(test, rank = 10)$x
pca_test <- as.data.frame(pca_test)
pca_test$classes <- test$classes

# QDA with PCA
qda_model_pca = qda(classes ~., data = pca_train, method = "mle")
train_pred_pca = predict(qda_model_pca, pca_train)$class
train_error = mean(train_pred_pca != train$classes)
test_pred_pca = predict(qda_model_pca, pca_test)$class
test_error = mean(test_pred_pca != test$classes)
cat("Train error rate: ", train_error, "\n")
cat("Test error rate: ", test_error)
```




## 4. Tree-based Models
**Using the discretized data, two types of tree-based models were tested. Bagging with 100 trees and m = 57 results in testing MSE of 0.03999343, and random forest with m=8 and 100 trees results in testing MSE of 0.03899001. **

### (1) Bagging
```{r}
# Bagging (Random forests with m=p) and 100 trees
bag.model = randomForest(classes~., data=train,
                           mtry=(ncol(train)-1),importance=TRUE, ntree=100)
summary(bag.model)
```

```{r}
# Prediction on training set for bagging
training_res = predict(bag.model, train)
cat("Training MSE: ", mean((training_res-train$classes)^2))
```

```{r}
# Prediction on test set for bagging
bag.predicted = predict(bag.model, test)

# Plot prediction performance
plot(bag.predicted, true)
abline (0,1)

# MSE on test set
cat("Testing MSE: ", mean((bag.predicted-true)^2))
```

### (2) Random Forest
```{r}
# Random forests with m=sqrt(p) and 100 trees. p=57, let m be 8
RF.model = randomForest(classes~., data=train,
                           mtry=8,importance=TRUE, ntree=100)
summary(RF.model)
```

```{r}
# Prediction on training set for bagging
training_res = predict(RF.model, train)
cat("Training MSE: ", mean((training_res-train$classes)^2))
```

```{r}
# Prediction on test set
RF.predicted = predict(RF.model, test)

# Plot prediction performance
plot(RF.predicted, true)
abline(0,1)

# MSE on test set
cat("Testing MSE: ", mean((RF.predicted-true)^2))
```




